* act-cutflow 
In this project I am trying to re-organize the cuts pipeline codes in
ACT and break them into reusable pieces. This will allow us to build
more sophisticated pipelines on top of what's existing easily. It also
allow us to analyze the impacts of each step applied to the pipeline 
independently. 
** Run the codes
The codes here depend on the todloop project which is basically a giant
loop to iterate through a list of TODs. To install todloop,
#+BEGIN_SRC sh
pip install git+https://github.com/guanyilun/todloop
#+END_SRC
Most of the scripts in this repo contain only definition of different
~Routine~ (more on this later). These routines can be added to a
pipeline that executes on a list of TODs. A simple pipeline is given
in ~test.py~ which consists of only two routines. The first routine
loads the TOD, and the second routine computes a few features of the
loaded TOD. To run it
#+BEGIN_SRC sh
python test.py
#+END_SRC
The (adapted version of) cut pipeline is defined in ~process_cuts.py~
(for running on hermes) and ~process_cuts_feynman.py~ (for running on
feynman). For example, on feynman,
#+BEGIN_SRC sh
python process_cuts_feynman.py
#+END_SRC
On hermes,
#+BEGIN_SRC sh
python process_cuts_hermes.py
#+END_SRC
This will run the cut pipeline on a list of 80 TODs with 60 TODs used 
for training and another 20 TODs used for validation. 

** Architecture
The basic unit in the analysis is called a ~Routine~. It represents a
set of operations related to a specific task or analysis. For
instance, some of the routines that are applied to each TOD before the
multi-frequencies analysis includes applying MCE cuts, planet cuts,
source cuts, detrends, applying filter cuts, etc. Each of these steps
will be a separate ~Routine~. In this way we can have a more
modularized design as all the routines are independent of the other
ones, and they are targeted at a specific task. This also makes the 
codes more readable and reusable. 

The way that different routines communicate with each other is through
a shared ~DataStore~ object. One can think of it as a giant dictionary (it
actually is) that every routine can access. A typical workflow is to
load some data associated with a key through the data store and
process it, the output can be stored under another key in the data
store so other routines can access it if needed.

Since most of the analysis related to cuts will involve looping
through a list of TODs, and this is taken care by the underlying
framework that this project is built on (called TODLoop). In this
framework, each routine has a few hooks to the different stages of the
analysis, for example, an empty routine looks like this.

#+BEGIN_SRC python
  class AnEmptyRoutine(Routine):
      def __init__(self, **params):
          Routine.__init__(self)
        
      def initialize(self):
          """Scripts that run before processing the first TOD"""
          pass

      def execute(self, store):
          """Scripts that run for each TOD"""
          pass

      def finalize(self):
          """Scripts that run after processing all TODs"""
          pass
#+END_SRC

~initialize~ contains scripts that will be ran before processing all
tods, ~execute~ function contains script that will be ran for each
TOD, and it is typically where most of the analysis is performed. It
also provides a reference to the shared data store so one can retrieve
relevant information that are provided by other routines. ~finalize~
function contains the codes that should run after looping through all
tods, kind of clean-up stages.

An actual working example of a routine is like this
#+BEGIN_SRC python
class FindJumps(Routine):
    def __init__(self, **params):
        Routine.__init__(self)
        self.inputs = params.get('inputs', None)
        self.outputs = params.get('outputs', None)
        self._dsStep = params.get('dsStep', None)
        self._window = params.get('window', None)

    def execute(self, store):
        tod = store.get(self.inputs.get('tod'))

        # find jumps
        jumps = moby2.libactpol.find_jumps(tod.data,
                                           self._dsStep,
                                           self._window)
        # store the jumps values
        crit = {
            'jumpLive': jumps,
            'jumpDark': jumps,
        }
        
        # save to data store
        store.set(self.outputs.get('jumps'), crit)
#+END_SRC
This showcases the typical structure of a routine. One first retrieves
the relevant data from the data store, and export the transformed data
back to the data store after processing.

The parameters are provided in a driver program (~process_cuts.py~), with
the relevant section that looks like
#+BEGIN_SRC python
# add a routine to find jumps in TOD
jump_params = {
    'inputs': {
        'tod': 'tod'
    },
    'outputs':{
        'jumps': 'jumps'
    },
    'dsStep': 4,
    'window': 1,
}
loop.add_routine(FindJumps(**jump_params))
#+END_SRC
Here ~loop~ refers to an underlying loop that will iterate over a list of
TODs.

** The Cut Pipeline 
Here is a rough sketch of some of the routines in the existing pipeline
and their whereabouts in this repository. 

|---------------------+-----------------+-------------+-------------------|
| steps applied       | moby2           | here        | name              |
|---------------------+-----------------+-------------+-------------------|
| cut mce             | process_cuts.py | cuts.py     | CutPartial        |
| cut planets         | process_cuts.py | cuts.py     | CutPlanets        |
| cut sources         | process_cuts.py | cuts.py     | CutSources        |
| cut glitches        | process_cuts.py | cuts.py     | CutPartial        |
| remove hwp          | process_cuts.py | cuts.py     | SubstractHWP      |
| remove mean         | process_cuts.py | tod.py      | TransformTOD      |
| detrend             | process_cuts.py | tod.py      | TransformTOD      |
| remove filter gain  | process_cuts.py | tod.py      | TransformTOD      |
| downsample          | process_cuts.py | tod.py      | TransformTOD      |
| find zero detectors | pathologies.py  | tod.py      | GetDetectors      |
| find jumps          | pathologies.py  | cuts.py     | FindJumps         |
| calibrate to pW     | pathologies.py  | tod.py      | CalibrateTOD      |
| analyze scans       | pathologies.py  | analysis.py | AnalyzeScan       |
| fourior transform   | pathologies.py  | tod.py      | FouriorTransform  |
| multi-freq analysis | pathologies.py  | analysis.py | AnalyzeDarkLF ... |
|---------------------+-----------------+-------------+-------------------|

** Files
- cuts.py: cuts related routines
- tod.py: tod related routines
- analysis.py: mainly the multi-freq analysis, also some temperature
  analysis, scan analysis, etc.
- utils.py: some utility functions such ~nextregular~ for fft
  preselection functions
- report.py: routines related to reporting the results of analysis
- features.py: design new features that may be useful
- process_cuts_feynman: the driver program for feynman, it defines the
  pipeline and specifies the parameters inputs for each routine
- process_cuts_hermes: same script for hermes
- process_cuts_features: experimental driver scripts to test
- test.py: test driver script 

** Status Quo
Currently the pipeline consists of the following routines (example output):
#+BEGIN_SRC 
2019-02-06 03:05:07,404 [INFO] TODLoop: Added routine: TODLoader
2019-02-06 03:05:07,404 [INFO] TODLoop: Added routine: CutSources
2019-02-06 03:05:07,404 [INFO] TODLoop: Added routine: CutPlanets
2019-02-06 03:05:07,405 [INFO] TODLoop: Added routine: RemoveSyncPickup
2019-02-06 03:05:07,405 [INFO] TODLoop: Added routine: CutPartial
2019-02-06 03:05:07,405 [INFO] TODLoop: Added routine: TransformTOD
2019-02-06 03:05:07,405 [INFO] TODLoop: Added routine: AnalyzeScan
2019-02-06 03:05:07,405 [INFO] TODLoop: Added routine: GetDetectors
2019-02-06 03:05:07,405 [INFO] TODLoop: Added routine: CalibrateTOD
2019-02-06 03:05:07,406 [INFO] TODLoop: Added routine: FindJumps
2019-02-06 03:05:07,406 [INFO] TODLoop: Added routine: FouriorTransform
2019-02-06 03:05:07,406 [INFO] TODLoop: Added routine: AnalyzeDarkLF
2019-02-06 03:05:07,406 [INFO] TODLoop: Added routine: AnalyzeLiveLF
2019-02-06 03:05:07,406 [INFO] TODLoop: Added routine: GetDriftErrors
2019-02-06 03:05:07,406 [INFO] TODLoop: Added routine: AnalyzeLiveMF
2019-02-06 03:05:07,406 [INFO] TODLoop: Added routine: AnalyzeHF
2019-02-06 03:05:07,407 [INFO] TODLoop: Added routine: Summarize
2019-02-06 03:05:07,407 [INFO] TODLoop: Added routine: PrepareDataLabel
#+END_SRC
The parameters that can be computed include
#+BEGIN_SRC 
['darkRatioLive',
 'corrLive',
 'corrDark',
 'kurtLive',
 'normLive',
 'kurtpLive',
 'normDark',
 'MFELive',
 'skewLive',
 'gainLive',
 'DELive',
 'gainDark',
 'jumpDark',
 'rmsDark',
 'jumpLive',
 'rmsLive',
 'darkSel',
 'skewpLive']
#+END_SRC
** Descriptions of Routines
A brief description of each of these routines and where to find it
|------------------+---------------------------------------------------+-------------|
| *Routine*        | *Description*                                     | *Location*  |
|------------------+---------------------------------------------------+-------------|
| TODLoader        | Load TOD into data store                          | todloop     |
|------------------+---------------------------------------------------+-------------|
| CutSources       | Remove sources from TOD data                      | cuts.py     |
|------------------+---------------------------------------------------+-------------|
| CutPlanets       | Remove planet from TOD data                       | cuts.py     |
|------------------+---------------------------------------------------+-------------|
| RemoveSyncPickup | Remove sync pickup from TOD data                  | cuts.py     |
|------------------+---------------------------------------------------+-------------|
| Cut Partial      | Remove glitches and MCE errors                    | cuts.py     |
|------------------+---------------------------------------------------+-------------|
| TransformTOD     | Downsampling, detrend, remove mean, etc           | tod.py      |
|------------------+---------------------------------------------------+-------------|
| AnalyzeScan      | Find scan freq and other scan parameters          | analysis.py |
|------------------+---------------------------------------------------+-------------|
| GetDetectors     | Find live and dark detector candidates            | tod.py      |
|------------------+---------------------------------------------------+-------------|
| CalibrateTOD     | Calibrate to pW using flatfield and responsivity  | tod.py      |
|------------------+---------------------------------------------------+-------------|
| FindJumps        | Find jumps and calculate jumpLive, jumpDark       | cuts.py     |
|------------------+---------------------------------------------------+-------------|
| FouriorTransform | Simple fourior transform                          | tod.py      |
|------------------+---------------------------------------------------+-------------|
| AnalyzeDarkLF    | Study dark detectors in low frequency, calculate  | analysis.py |
|                  | corrDark, normDark, gainDark                      |             |
|------------------+---------------------------------------------------+-------------|
| AnalyzeLiveLF    | Study live detectors in low frequency, calculate  | analysis.py |
|                  | corrLive, normLive, gainLive, darkRatioLive       |             |
|------------------+---------------------------------------------------+-------------|
| GetDriftErrors   | Study the slow modes and calculate DELive         | analysis.py |
|------------------+---------------------------------------------------+-------------|
| AnalyzeLiveMF    | Study the live detectors in mid frequency,        | analysis.py |
|                  | calculate MFELive                                 |             |
|------------------+---------------------------------------------------+-------------|
| AnalyzeHF        | Study both the live and dark detectors in high    | analysis.py |
|                  | frequency and calculate rmsLive, kurtLive,        |             |
|                  | skewLive, rmsDark                                 |             |
|------------------+---------------------------------------------------+-------------|
| Summarize        | Get the results from previous routine and combine | report.py   |
|                  | them into a dictionary                            |             |
|------------------+---------------------------------------------------+-------------|
| PrepareDataLabel | Load analysis results and sel from Pickle file    | report.py   |
|                  | to create an h5 file which can be supplied to     |             |
|                  | the mlpipe pipeline                               |             |
|------------------+---------------------------------------------------+-------------|
| JesseFeatures    | Calculate the 4 features that Jesse came up with  | features.py |
|------------------+---------------------------------------------------+-------------|

** Major Differences
While breaking the moby2 cuts codes into individual components. There
are some changes made to the pipeline for exploration. Here is a list
of them:

- Pre-selection
Pre-selection in moby2 requires a fine tuning of parameters. In particular, 
the ~presel_by_group~ function alone requires 5 parameters to tune. The
~presel_by_median~ function requires 3 parameters to tune. Since our objective
is to reduce the human intervention as much as possible, the pre-selection
is removed. The idea is to use some smarter algorithms to replace this 
fine tuning process. More on this later. 

- Partial statistics
For the high frequency analysis, the original pipeline in moby2 performs
the analysis on each chunk in the scan (between turning points). This is
not enabled for now for simplicity. 

** Machine Learning
The ~PrepareDataLabel~ routine makes way for the machine learning
study by preparing an h5 file with all the necessary data to train
machine learning models that can directly be supplied to the machine
learning pipeline codes ([[https://github.com/guanyilun/mlpipe][mlpipe]]). An example output from this
machine learning pipeline is shown below
#+BEGIN_SRC 
  == VALIDATION RESULTS: ==
  
    epoch    batch  model               loss      base    accuracy    tp    tn    fp    fn    precision    recall        f1     time/s
  -------  -------  ---------------  -------  --------  ----------  ----  ----  ----  ----  -----------  --------  --------  ---------
        0        0  KNNModel-3       2.05077  0.422877    0.940625  6864  9089   699   308     0.907576  0.957055  0.931659  2.09715
        0        0  KNNModel-7       1.7005   0.422877    0.950767  7088  9037   751    84     0.904197  0.988288  0.944374  2.09129
        0        0  RandomForest-5   1.41335  0.422877    0.95908   7154  9112   676    18     0.913665  0.99749   0.95374   0.0665109
        0        0  KNNModel-5       1.81862  0.422877    0.947347  7012  9055   733   160     0.905358  0.977691  0.940135  2.08214
        0        0  XGBoost          1.38688  0.422877    0.959847  7157  9122   666    15     0.914866  0.997909  0.954585  0.0552425
        0        0  DecisionTree     1.86952  0.422877    0.945873  6862  9180   608   310     0.918608  0.956776  0.937304  0.0112839
        0        0  RandomForest-20  1.40724  0.422877    0.959257  7153  9116   672    19     0.914121  0.997351  0.953924  0.178965
        0        0  SVCModel         1.76771  0.422877    0.948821  7172  8920   868     0     0.89204   1         0.94294   5.48178
        0        0  RandomForest-10  1.40521  0.422877    0.959316  7157  9113   675    15     0.913815  0.997909  0.954012  0.102943
#+END_SRC
It shows that even after removing some major fine tuning steps we can
achieve reasonably good results. This is a hint that the existing cut
pipeline can be simplified furthur with the help of machine learning
techniques.


